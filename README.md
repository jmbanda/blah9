# BLAH 9 Proposal: Benchmarking and evaluation of automated phenotype definition extractions with large language models

Building one the work performed at [BLAH8](https://docs.google.com/presentation/d/1zfoxt6Kif8lygkLqJPGHxMpKzW7S04ULEcM9pWpcXpw/edit?usp=sharing), (soon) to be published at [Genomics and Informatics](https://www.researchsquare.com/article/rs-4798033/v1) and presented at the [OHDSI 2024 symposium](https://www.ohdsi.org/2024showcase-52/), we propose to create a formal benchmark suite for the evaluation of automated extraction of phenotype defitions. We have expanded our phenotype definitions from 10 to 25 since BLAH8, and we will focus during BLAH9 to create a set of open-source jupyter notebooks that leverage Hugging Face LLM models to create a benchmark of open-source LLMs. We will use our current results from GPT3.5 and GPT4 as baseliness and will add at least results for five or more open-source models found via Hugging Face. The idea of having a benchmark is to have some baseline results and have all code available for reproducibility. With an advantage of the benchmark suite being jupyter notebooks is that other users can add their own LLMs to Hugging Face, and then just run our notebooks to evaluate and benchmark their methods. This approach will allow easier community participation, as well as incentivize the submission of custom LLMs to the Hugging Face platform and have a standard and transparent benchmark/feedback mechanism.
